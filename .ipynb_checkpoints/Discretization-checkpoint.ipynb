{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is feature discretization\n",
    "\n",
    "Suppose we have a feature that is either countinuoous or have large range. In this settings we cannot use some models directly, for example naive bayes would need one parameter for each possible value and would still know nothing about values that were not seen in training, and that could be a lot values since the range is big or uncountable.\n",
    "\n",
    "Solution is to split values into $K$ bins. What basically transforms feature into $K$-valued ordinal value. And there are multiple ways how we could do this, and i havent found the best on so I just will describe what I have found.\n",
    "\n",
    "## Goal\n",
    "The goal can be formulated like this: given ordered sequence $S=\\lbrace{a_i}\\rbrace$, return ordered sequence of intervals $T=\\lbrace (begin_i,end_i)\\rbrace$\n",
    "\n",
    "It is obvious that such definition is uselless, because returning empty sequence would be enough. But I havent't figured out how to judge which set of interval (lets call it solution from now on) is better. It looks like each method that i will describe later will have its own metric that it will try to optimize. And we can always to the obvious check on final classifier metric on validation set.\n",
    "\n",
    "# Most simple approach\n",
    "\n",
    "There are two intuitive methods, one is to split entire range to $K$ equaly large interval that is called ** equal width **\n",
    "this method can generate very sprase intervals and is sensitive to outlayers\n",
    "**equal frequency** on the other hand picks intervals with equal number of elements, for example first 10% of data would be in first interval, second 10% in other. That approach is in my opinion more robust, but it might produce too dense intervals and it does not guarantee that points from different intervals are far away\n",
    "\n",
    "## Can we combine those approaches\n",
    "\n",
    "Of course we can, for example if you have too little parametrs in grid seach, or you would like to base your academic career on studying complex heuristics. One way is to apply both condition at the same time, for example interval has to be at least $w$ long and contain at least $c$ elements, or other way around. Or we can define for each length minimal number of points :-)\n",
    "Another way is to define split criteria if $\\alpha c + \\beta w \\ge \\gamma$ if both coefficents are positive the partition ing is well defined, but if they signs differs, the split function is non monotonic, that means it might be false, then true and then again false. So we need to accept either first or last split or median or whatever. You might ask why having $\\beta$ negative, the motivation for it is to do not split while density is good enough (where the density is depends on values of $\\alpha$ and $\\beta$)\n",
    "\n",
    "# Less simple approach\n",
    "\n",
    "The approach described here is base on using of the shelf clustering algorithm and deriving split points by how clusters are made.\n",
    "  * Computation complexity skyrockets\n",
    "  * It is not clear how to choose intervals once clustering is done\n",
    "  \n",
    "## k-means\n",
    "In this clustering scheme, cluster membership is determined by what centroid (cluster center) is closest.\n",
    "So the splitting point would be halfway between two centroids.\n",
    "\n",
    "## Agglomerative clustering\n",
    "Agglomerative clustering starts with as many cluster as there is points and iteratively merges two closes clusters that forms closest pair according to some metric, and ther are  few of them:complete linkage, average-linkage.\n",
    "**recursive partitioning ** goes in reverse order but basically is the same.\n",
    "\n",
    "It is obvious that everytime the cluster will form one interval, but if whole space is not covered we have to decide where will be the spliting point, we can compute cluster center and proceed as in k-means case, or split unknown interval in half, or according to cluster variance or density, so the dense cluster is supposed to not reach out too much in unknown.\n",
    "\n",
    "# chi-square\n",
    "Similar to agglomerative clustering but joins two clusters iff chi-square test of their union is belowe some threshold. That means if points are similar there. But $chsq= {\\sum (E(X)-X)^2 \\over E(X)}$ and i do not know why not just variance was used..\n",
    "\n",
    "\n",
    "Thank you for reading, it is all for now, i will probably add some implementation later, hopefully it is all in sci-kit learn somewhere.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
